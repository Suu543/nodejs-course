# Buffer and Stream in Node.js

<img src="https://miro.medium.com/max/800/1*_TgFYjypgLulDB-a6pCeMg.png" />

## What is a buffer in NodeJS?
`Buffer`는 `이진수(Binary) 데이터`이고, 이 이진 데이터는 오직 `0`과 `1`로만 구성된다. `Javascript Language`에는 `Binary Data Stream`을 읽거나 조작할 수 있는 `mechanism`이 존재하지 않는다. `Octet streams in TCP streams, file system operations, and other context`와 상호작용 하기 위해 `Node.js API`의 일부로 `Buffer Class`를 만들었다. `Buffer`라는 말은 우리가 영상을 볼 때 로딩이 걸릴 때 버퍼링이라는 말과 동일한 말이고, `Buffer`와 항상 붙어 다니는 말이 `Stream`이다. 이 용어 또한 우리가 많이 듣던 스트리밍의 의미이다. 데이터를 전송한다는 의미이다. 이 정도의 내용만 알고 우리는 `Buffer`와 `Stream`을 이해하고 있다고 말 할 수 있을까? 나는 그렇지않다 생각해서 이 글의 작성을 시작했다.

## Why does a buffer matter?
컴퓨터는 정보를 저장하고 탐색할 때 여전히 `이진 데이터(Binary Data)`에 의존한다. `Buffer`는 `stream of binary Data`를 다루기 위해 존재한다. 이 포스트를 작성하기 몇일 동안 `Javascript`를 사용하는데 왜 `Buffer`를 알아야 하지 생각했다 왜냐하면 `Javascript`은 `High-level-language`이고, 한 번도 `Buffer`를 직접 구현해 이용한 적이 없기 때문이다. 아이러니하게도, 많은 데이터를 다뤘고 애초에 컴퓨터는 이 데이터를 이진 데이터로 인식하는데 왜 나는 `Buffer`를 이용한 적이 없을까 생각했고, 이 의문을 해소하기 위해 보다 더 자세하게 찾아보았다.

`Buffer`는 본질적으로 `이진 코드(Binary Code)`의 저장 단위이면서 `이진 코드(Binary Code)`를 전송 전후에 일시적으로 정하는데 사용되기도 한다. 이러한 `Buffer`의 이용성을 생각했을 때 `Buffer`는 굉장히 강력하다고 생각했다 그 이유는 앞서 여러 번 언급한 것처럼 모든 컴퓨터가 처리하는 데이터는 `이진 데이터(Binary Data)`이기 때문이다.

그렇다면 `Buffer`가 존재하지 않는다면 어떤 문제가 발생할까? 전송하는 데이터는 `이진 코드(Binary Code)`로 나눈 후 이 나눠진 `이진 코드(Binary Code)`를 즉시 전송하고, 이 `이진 코드(Binary Code)` `수신자(Receiver)`는 큰 덩어리의 `이진 코드(Binary Code Chunks)`를 수신하자마자 바로 이 코드를 사용하기 시작해야 한다 왜냐하면 `이진 코드(Binary Code)`를 저장할 방법이 없기 때문이다. 더욱 더 깊은 `Buffer`이해를 위해 `Stream`을 먼저 알고 넘어가자 왜냐하면 데이터의 전송과정은 `Stream`과 더 밀접히 관련 있기 때문이다.

인터넷은 데이터 혹은 정보를 한 위치에서 다른 위치로의 전송에 기반을 두고 동작한다. 우리가 `Online`상에서 행하는 모든 작업은 한 컴퓨터에서 다른 컴퓨터로의 데이터 이동과 밀접히 연관이 있다. `Online`에서 이 모든 데이터의 이동을 관장하는 역할을 하는 도구를 `Stream`이라 생각할 수 있다.

일반적으로 개발을 하는 입장에서 데이터 전송의 내부 동작에 대해 크게 신경 쓰지 않아도 잘 동작하기 때문에 크게 신경 쓰지 않을 수 있다. 하지만, 모든 상황에서 데이터의 송수신이 발생하기때문에, 개발자는 `Stream`을 이해하고 이 또 본인이 원하는 방식으로 데이터 전송을 관리할 수 있어야 한다고 생각한다. `Javascript and Nodejs`를 이용해서 `Stream`이 어떻게 사용되는지 알아보자.

### Stream을 이용하지 않으면 두 가지의 비효율성이 발생한다.
1. 대용량의 `데이터 전송(Data Transportation)`에 굉장히 비효율적이다 그 이유는 한 파일을 전송하기 전 그 파일 전체를 읽어야 전송될 수 있는데, 용량이 큰 파일인 경우 그 파일 전체를 다 읽는데 시간이 오래 걸리기 때문이다.
2. 이 비효율적으로 전송된 데이터가 수신자에게 도착해도 수신자가 데이터에 접근하려면 해당 파일 전체를 메모리에 기록해야 하기 때문에 비효율적이다.

- 아래 작성한 코드는 현재 `Stream`을 사용하지 않았다. 아주 작은 크기의 데이터를 다루는 경우에는 크게 문제가 되지 않지만, 데이터의 크기가 커질 수록 바로 위 언급한 두 비효율성의 문제가 발생한다.

```javascript
const fs = require('fs');

fs.readFile(filePathOne, (err, data) => {
   fs.writeFile(filePathTwo, data);
})
```

## What is Streams?
`Streams`은 기본적으로 데이터를 덩어리로 쪼개서 전송하고, 기본 동작 방식은 `Reading and Writing`이다. `A File` 로부터 데이터를 읽고 `(Reading)`, `B File` 읽은 데이터를 작성한다`(Writing)`.

`Streams`은 효율적인 메모리 공간의 사용을 가능하게 한다. 예를 들면, 매우 큰 하나의 파일이 하나 있고, `A`가 이 파일을 `B`에서 `Encoding`을 하고 `C`로 `Encoding`된 파일을 전송하고 싶은 상황을 생각해보자. 앞서 언급한 것 처럼 `Stream`은 데이터를 `Chunk by Chunk`로 쪼개서 전송한다고 했다. 이 전송 방식을 생각했을 때, `A`가 큰 하나의 파일을 한 번에 `B`에게 전송하는 것이 아니고 `Chunk by Chunk`로 쪼개서 한 덩이씩 전송한다, 그럼 `B`는 `한 덩이(Chunk)`의 데이터를 수신하고 이 데이터만 `Encoding`해 저장하지 않고 바로 `C`로 전송한다.

더 쉽게 실생활에 묘사하면, 빵집에서 100개의 팥빵을 만들어야 하는데 이 빵집의 오븐의 크기가 작아서 빵이 한 번에 하나만 만들어진다. 이때 가장 효율적으로 기다리는 100명의 고객에게 빵을 주는 방법은 100개의 빵이 나올 때까지 기다려서 100명에게 동시에 제공하는 것이 아닌, 하나가 나오면 줄 선 순서대로 바로바로 빵을 주는 방식이다. 이렇듯, A가 100개의 팥빵 반죽을 1인분 단위로 하나씩 쪼개서 B에게 하나씩 전송하면 B는 받은 반죽을 바로 구워서 나오는 대로 가지고 있지 않고 바로 C에게 전달해준다고 이해하면 된다. 여기서 B를 메모리는 팥빵 한개이다. 그러므로, B가 100개의 팥빵의 메모리를 가지지 않고도, 100개의 팥빵을 효율적으로 전달할 수 있다.

`Stream`의 `Chunk`의 크기는 프로그래머가 수동으로 설정이 가능하다.
`4`개의 `Streams Type`이 있다.

1. Readable(“Source”)
2. Writable(“Destination”)
3. Duplex(“Readable & Writable”)
4. Transform(modifies the data midstream)

<img src="https://miro.medium.com/max/1400/1*a_pAIhk5Nn8FHoMt7W0BWQ.png" />

## Source and Destination 개념
`Readable and Writable streams`은 `streams`의 꽃과 같은 존재이다. `Readable Streams`은 `source stream`이라 생각 할 수 있다. 왜냐하면 `Stream`을 사용하던 안 하든 우선은 보내려는 데이터가 어떤 데이터인지 읽어야 보낼 수 있기 때문이다. `Writable Streams`은 `destination stream`이라 생각 할 수 있다. 왜냐하면 `Stream`을 사용하던 안 하든 데이터가 적히는 동작은 읽은 데이터를 받은 곳에서 이뤄지기 때문이다.

`Nodejs File System`을 이용해 `Stream`을 구현해보면.
```javascript
const fs = require('fs');

fs.createReadStream(filePathOne)
  .pipe(fs.createWriteStream(filePathTwo));
```

코드에서, `File System module`의 `createReadStream method`는 `filePathOne` 으로부터 데이터를 `Chunk by Chunk`로 가져와 사용하기위해 사용하고, `the pipe method` 는 `createReadStream method`에서 전송된 `Chunk by Chunk`를 결합하고, `createWriteStream`을 사용해 `Chunk by Chunk`로 데이터를 `filePathTwo`로 전송한다. 즉석에서 데이터를 작성하는 경우 `createWriteStream`만 사용하여 데이터를 `Stream`하는 것이 가능하다. 또한, 데이터를 다른 `Destination`으로 전송하지 않는 경우 `readable stream`만 사용할 수 있다. 당연한 거지만, `Reading and Writing` 순서가 변경되면 동작하지않는다.

요약: Readable(= source) streams → Writable(= destination) streams

## Duplex Streams
`Duplex`는 이중을 의미한다. 하나의 `Duplex Stream`은 두 개의 `Channels`로 구성된다. 하나의 `Channel`은 데이터를 수신하고 다른 하나의 `Channel`은 데이터를 송신한다 (상황마다 두 채널 모두 송수신의 동작을 한다). 각 채널은 각자의 `Buffer`를 가지고 있다. 즉, 두 채널이 동시에 정보를 교환 할 수 있다는 말이다.

Pattern: a.pipe(b).pipe(a)

이 패턴은 `Duplex Stream`을 가능하게 만든다. 예를 들면, 한 `Connection`에 두 대의 컴퓨터가 연결이 되었고, 두 컴퓨터 모두 서로에게 데이터를 송수신 할 수 있다. `Duplex Streams`은 `online`에서 가장 흔하게 사용하는 `Stream`이다. 대표적으로 `Network Socket`과 같은 기술이 데이터 송수신을 위해 `Duplex Steams`을 이용한다 왜냐하면 실시간으로 여러 대의 컴퓨터가 한 `Connection`에서 데이터를 주고받아야 하기 때문이다. 이러한 특성때문에, `Duplex Streams`은 `TCP(Transmission Control Protocol) and IP (Internet Protocol)`이용하는 거의 모든 `Network Devices`를 연결하는 데 사용된다.

## Transform Streams
`Transform Stream`은 `Midstream`으로 전송되는 데이터를 변경하는 `Stream`이다. 앞서 언급한 `A`에서 `C`로 데이터를 전송하기 전에, `B`에서 `Encoding`하고 `C`로 보내는 것을 생각해보면 된다. 여기서 `MidStream`은 `B`이다. `Nodejs`의 `zlib`와 `crypto modules`을 통해 `Transform Stream`이 무엇인지 설명해보겠다.

```javascript
const fs = require('fs');
const zlib = require('zlib');
const crypto = require('crypto'); 
//Zip
fs.createReadStream(filePathOne)  
  .pipe(zlib.createGzip())  
  .pipe(crypto.createCipher('aes192', 'Crypto_secret'))      
  .pipe(fs.createWriteStream(filePathTwo));  
//Unzip
fs.createReadStream(filePathTwo)      
  .pipe(crypto.createDecipher('aes192','Crypto_secret')) 
  .pipe(zlib.createGunzip())
  .pipe(fs.createWriteStream(filePathThree));
```

이 코드는 처음 `filePathOne`으로 부터 파일을 읽고, `filePathTwo`에 읽은 파일을 작성한다. 하지만, 주목할 점은 `filePathTwo`에 읽은 파일을 `Chunk by Chunk`로 전송하기 전에, `createGzip method`를 이용해 `Chunk`를 압축하고 `createCipher method`를 이용해 압축한 `Chunk`를 암호화해서 `filePathTwo`에 `Chunk by Chunk`로 전송된다. 여기까지가 `Zip` 부분의 동작이고.

`Unzip` 부분은 현재 `filePathTwo`에 `Write`되어있는 압축되고 암호화된 파일을 `createReadStream`을 통해 `Read`를 한 후 압축 해제 및 암호해독을 `Chunk by Chnuk`로 해 최종적으로 `filePathThree`에 암축 해제 및 암호 해독된 `Chunk`들을 `Write` 하게 된다.

이러한 과정의 `Data-Stream`을 `Transform Stream`이라 칭한다. `서로 고립된 상태에서 (createReadStream/createWriteStream)`과 동일한 `Method`를 사용해 `transform stream`의 `Source/Destination`을 위 코드와 같이 구현 할 수 있다. 전체적으로, `Stream`의 역할을 이해하는 것이 `Buffer`를 이해하는데 큰 도움이되고 뿐만 아니라 프로그래머의 관점에서 큰 도움이 될 것 같아서 추가 작성했다. 자 이제 다시 `Buffer`로 넘어가 보겠다.

<img src="https://miro.medium.com/max/1400/1*upay4GEg3_krMdBL0ZxaoA.png" />

## Buffers
여기까지 왔다면 `Streams`의 기능을 어느 정도 이해했을 거라 생각한다. `Buffer`는 앞서 언급했듯 `이진 코드(Binary Code)`에 `일시적 저장공간(Temporary Storage)`을 제공한다고 했다. 특히, `Buffers`는 `Binary Code`가 전송을 끝날 때까지 전송하려는 `Binary Code`를 모아두는 역할을 한다. 전송이 끝났다는 의미를 조금 더 자세하게 묘사하면, `수신자(Receiver)`에게 끊킴 없이 전송되는 수준의 정도이다. 우리는 어느 정도의 `Rate`으로 `Binary Code`가 전송되는지 모르기 때문에 `Binary Code`를 모아두는 역할은 정말 중요하다. 데이터의 타입이나 사용 목적에 따라 다르겠지만, 우리는 일관성 있는 `Binary Code`의 전송을 원한다. 예를 들면, 영화를 `Streaming`할 때 데이터 전송의 문제로 자주 끊어지는 영화를 보기를 원하지 않는다. 여기서 말하는 일관성이 앞서 언급한 끊킴 없는 데이터의 전송이라 생각하면 된다. 또한 우리는 영화를 재생하기 전에 모든 데이터를 필요로 할 수 있다 왜냐하면 `Binary Data`가 성공적으로 다 전송되었음을 보장하고 싶기 때문이다.

이렇듯 일관성 있고 끊킴 없는 데이터 전송 및 영화를 재생하기 전 모든 데이터의 준비와 같은 상황에 `Buffers`를 이용하면 우리가 원하는 결과를 얻을 수 있다.

실생활의 예시로 `Buffers`의 동작을 설명해보겠다. 학교를 다닐 때 갔던 수학여행을 생각해보자. 우선 수학여행을 간다고 동의한 학생이 모두 학교에와야지만 관광버스가 출발한다 그리고 당연히 학생 수가 많아서 여러 대의 버스를 요구한다. 1반 학생은 1반 버스를 타고 가고, 2반 학생은 2반 버스를 타고 간다. 이 말은 1반 학생이 다 오면 2반 학생의 여부와 상관없이 1반 학생의 버스는 출발한다는 의미이다. 그리고 1 반 학생 버스가 `목적지(Destination)`에 도착하면, 다른 반 버스가 다 도착할 때까지 기다려야 한다.

이 예시에서, 각 버스가 출발하기 전 각 반 학생들이 다 오길 기다리는 행위를 하는 `Loading Area`를 `Buffer`라 생각하면된다. 이 행위의 목적은 각 반의 수학 여행을 신청한 학생이 모두 성공적으로 수학 여행지에 갈 수 있음을 보장한다. 유사하게, 먼저 출발 한 버스가 도착해 다른 버스를 기다리는 `Loading Area`또한 `Buffer`라 간주 할 수 있다 왜냐하면 모든 버스가 성공적으로 도착함을 보장하기 때문이다.

- Loading Area(운동장 or 도착지): Buffer
- 각 반의 학생과 선생님: Bits of Data
- 각 버스: Binary Chunks

```javascript
const decoder = new TextDecoder();
const buffer = new ArrayBuffer(32);

console.log(buffer.byteLength); // 32
console.log(buffer[3]); // undefined

const bufferView = new Uint8Array(buffer);
bufferView[3] = 65 // ASCII Char for 'A'
console.log(bufferView[3]); // 65
console.log(decoder.decode(bufferView)[3]) ; // 'A'
```

`JS`는 `ES6`의 `Typed Array`를 소개하기 이전에 `Raw Binary Data`를 처리하기 위한 어떠한 것도 제공하지 않았다. 현재는 `TypedArray`를 통해 `Raw Binary Data`에 접근 할 수 있다. `TypedArray`는 `Buffer`를 볼 수 있게 해주는 `object`와 같은 `Array`이다. `4 ~ 5 번` 줄 에서 보이는 것 처럼 `Buffer` 자체에 직접적으로 접근해 값을 확인하고 조작할 수 없다. 대신에, `Buffer`에 접근하기 위해서, `TypedArray`의 `Instance`인 `bufferView` 라는 `view`를 생성해야한다. 7번 줄에서 볼 수 있듯 여기서 묘사한 `TypedArray`의 `Type`은 `Uint8Array`이다. `Uint8Array`의 의미는 `bufferView`라는 `Array`에 `UTF-8 characters`가 포함되어있음을 기대하는 것이다.

`UTF-8`은 (Uniform Transformation Format)의 줄임말이고 의미는 `1 ~ 4 개`의 `8비트 블록`을 사용해 각 문자를 `Encode`한다는 의미이다. `UTF-8`은 수 많은 데이터 `Encoding` 방식 중 하나이다. `bufferView`에서 허용 가능한 문자 코드를 `Buffer`에 추가하고 이를 `Decode`함으로인해 10번 줄의 결과값처럼 실제 데이터를 확인 할 수 있다.

`Empty Buffer`를 생성해 값을 채우는 것 대신, 일부 데이터로부터 `Buffer`를 생성하려면 `JS’s TextEncoder`를 이용해 구현 할 수 있다.

```javascript
// buffer class는 Node에서 Global class라서 일반적으로 require할 필요는 없다.
const Buffer = require('buffer').Buffer;

// UTF-8
const buffer = new Buffer('Hello World');

// Buffer data에서 String 데이터로 변환
bufferText = buffer.toString();

console.log(bufferText); // Hello World
```

`Node.js`를 사용하면 `Buffer`를 조작 할 수 있으며 실제로 `Abstraction`을 제공해준다. 우선 `npm install buffer`를 하고, 2번 줄 처럼 `require buffer`를 한다. 그리고, 5번 줄 처럼 `buffer`의 `instance`를 하나 생성한다. 이제 5번 줄의 `buffer`라는 변수는 `buffer class`의 `method`를 이용할 수 있다. 예를 들면, 8번 줄의 `toString()`과 같은 `method`이다. `toString() method`는 `buffer → string`으로 데이터를 변환해준다. 그리고, `utf-8`을 설정하지 않았는데 어떻게 `utf-8`로 설정이 되어있는 궁금 할 수 있는데. 그 이유는 `Node.js`의 `buffer`는 `JS`의 `Uint8Array`의 `subclass`이기 때문에 `utf-8`이 자동으로 설정된다. 추가로 `Buffer overflow`에 관해 공부해보기를 추천한다.

몇 가지 연습을 해보자.
Nodejs의 built-in module인 string_decoder를 사용해보자. string_decoder는 a stream of Binary Data( a buffer object)를 String으로 변환해주는 역할을 하는 class이다.

```javascript
const { StringDecoder } = require('string_decoder');
const decoder = new StringDecoder('utf-8');

const cent = Buffer.from([0xC2, 0xA2]);
console.log(cent);
console.log(decoder.write(cent));

const euro = Buffer.from([0xE2, 0x82, 0xAC]);
console.log(euro)
console.log(decoder.write(euro));

// console.log(cent); // <Buffer c2 a2>
// console.log(decoder.write(cent)); // ¢
// console.log(euro); // <Buffer e2 82 ac>
// console.log(decoder.write(euro)); // €
```

`decoder.write() method`는 지정된 `Buffer`를 `Decoded String`으로 반환하고 (현재 `UTF-8 format`을 기준으로), `argument`로 `buffer` 값을 받는다.

`Buffer.from() method`는 지정된 `string, array` 혹은 `buffer`로 구성된 새로운 `buffer`를 생성한다 `(array 안의 값들[ ])`. 현재 위 코드의 경우 from의 `array[]`의 값을 한 번에 모두 정의해서 `Buffer`가 `Temporary Storage` 역할을 한다는 것이 명확하지 않아 보일 수 있다.

이 코드를 아래와 같이 작성해보면,

```javascript
const { StringDecoder } = require('string_decoder');
const decoder = new StringDecoder('utf8');

decoder.write(Buffer.from([0xE2]));
decoder.write(Buffer.from([0x82]));
console.log(decoder.end(Buffer.from([0xAC])));
```

`decoder.write()`의 다른 특징은 `nodejs`의 `HTTP module`의 `response.write()` 는 `response.end()`가 나오기 전까지 동작을 완료하지 않은 체로 머물러있는 것 처럼 `decoder.write()` 또한 `decoder.end() method`가 나오기 전까지 동작을 완료하지 않은 체로 머물러있다.


